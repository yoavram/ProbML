{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Py4Eng](../logo.png)\n",
    "\n",
    "# Gated  Recurrent Unit\n",
    "## Yoav Ram\n",
    "\n",
    "In this session we will expand over RNN with GRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax 0.4.30 gpu\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import jax \n",
    "import jax.numpy as np\n",
    "print('jax', jax.__version__, jax.default_backend())\n",
    "import optax # pip install optax\n",
    "\n",
    "from collections import Counter\n",
    "from random import uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "In developing this RNN we will follow [Andrej Karpathy](http://cs.stanford.edu/people/karpathy/)'s [blogpost about RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness) ([original code gist](https://gist.github.com/karpathy/d4dee566867f8291f086) with BSD License).\n",
    "\n",
    "The data is just text data, in this case Shakespear's writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 99993\n",
      "Number of unique characters: 62\n",
      "Number of lines: 3298\n",
      "Number of words: 15893\n",
      "\n",
      "Excerpt:\n",
      "********\n",
      "That, poor contempt, or claim'd thou slept so faithful,\n",
      "I may contrive our father; and, in their defeated queen,\n",
      "Her flesh broke me and puttance of expedition house,\n",
      "And in that same that ever I lament this stomach,\n",
      "And he, nor Butly and my fury, knowing everything\n",
      "Grew daily ever, his great strength and thought\n",
      "The bright buds of mine own.\n",
      "\n",
      "BIONDELLO:\n",
      "Marry, that it may not pray their patience.'\n",
      "\n",
      "KING LEAR:\n",
      "The instant common maid, as we may less be\n",
      "a brave gentleman and joiner: he that finds u\n"
     ]
    }
   ],
   "source": [
    "filename = '../data/shakespear.txt'\n",
    "with open(filename, 'rt') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"Number of characters: {}\".format(len(text)))\n",
    "print(\"Number of unique characters: {}\".format(len(set(text))))\n",
    "print(\"Number of lines: {}\".format(text.count('\\n')))\n",
    "print(\"Number of words: {}\".format(text.count(' ')))\n",
    "print()\n",
    "print(\"Excerpt:\")\n",
    "print(\"*\" * len(\"Excerpt:\"))\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating \n",
    "- a list `chars` of the unique characters\n",
    "- `data_size` the number of total characters\n",
    "- `vocab_size` the number of unique characters\n",
    "- `int_to_char` a dictionary from index to char\n",
    "- `char_to_int` a dictionary from char to index\n",
    "and then we convert `data` from a string to a NumPy array of integers representing the chars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = list(set(text))\n",
    "data_size, vocab_size = len(text), len(chars)\n",
    "\n",
    "# char to int and vice versa\n",
    "int_to_char = dict(enumerate(chars)) #  == { i: ch for i,ch in enumerate(chars) }\n",
    "char_to_int = dict(zip(int_to_char.values(), int_to_char.keys())) # { ch: i for i,ch in enumerate(chars) }\n",
    "\n",
    "def onehot_encode(text):\n",
    "    ints = [char_to_int[c] for c in text]\n",
    "    ints = np.array(ints, dtype=int)\n",
    "    return jax.nn.one_hot(ints, vocab_size)\n",
    "\n",
    "def onehot_decode(data):\n",
    "    ints = data.argmax(axis=1).tolist()\n",
    "    chars = (int_to_char[k] for k in ints)\n",
    "    return str.join('', chars)\n",
    "\n",
    "X = onehot_encode(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU model\n",
    "\n",
    "The GRU extends RNN. It avoids the vanishing gradient problem for the vanilla RNN, and is more efficient than LSTM (long-short time memory).\n",
    "To compute the update to the hidden memory layer $h_t$, it first computes a _reset gate_ $r_t$ and an update gate $z_t$ that are used to interpoltate between the candidate memory $\\tilde h_t$ and the next $h_t$.\n",
    "\n",
    "- $x_t$ is the $t$ character, one-hot encoded and a 1D array of length `vocab_size`.\n",
    "- $h_t$ is the state of the hidden memory layer after seeing $t$ characters, encoded as a 1D array of numbers (neurons...)\n",
    "- $r_t$ is the _reset gate_\n",
    "- $z_t$ is the _update gate_\n",
    "- $\\tilde h_t$ is the candidate hidden memory\n",
    "- $\\widehat y_t$ is the prediction of the network after seeing $t$ characters, encoded as a 1D array of probabilities of length `vocab_size`\n",
    "- $\\sigma(x)$ is the sigmoid/logistic function\n",
    "- $\\circ$ is the Hadamard or element-wise product, $x \\circ y = (x_1 y_1, \\ldots x_n y_n)$.\n",
    "\n",
    "The model is then written as:\n",
    "$$\n",
    "z_t = \\sigma{\\left(W_x^z x_t + W_h^z h_{t-1} + b_z\\right)}\n",
    "$$\n",
    "$$\n",
    "r_t = \\sigma{\\left(W_x^r x_t + W_h^r h_{t-1} + b_r\\right)}\n",
    "$$\n",
    "$$\n",
    "\\tilde h_t = \\tanh{\\left(W_x^h x_t + W_h^h (r_t \\circ h_{t-1}) + b_h\\right)}\n",
    "$$\n",
    "$$\n",
    "h_t = (1-z_t) \\circ h_{t-1} + z_t \\circ \\tilde h_t\n",
    "$$\n",
    "$$\n",
    "\\hat y_t = \\mathrm{softmax}\\left(W_h^y h_t + b_y\\right)\n",
    "$$\n",
    "$$\n",
    "x_{t+1} \\sim \\mathrm{Cat}(\\hat{y}_t)\n",
    "$$\n",
    "\n",
    "and we set $h_0 = (0, \\ldots, 0)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This operation will be performed by our `step` function.\n",
    "\n",
    "The `feed_forward` function will loop over a sequence of $x=(x_1, x_2, \\ldots, x_k)$ of some arbitray size - similar to batches in the FFN and CNN frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(params, x, h):\n",
    "    Wxz, Whz, Wxr, Whr, Wxh, Whh, Why, bz, br, bh, by = params\n",
    "    z = jax.nn.sigmoid(Wxz @ x + Whz @ h + bz)\n",
    "    r = jax.nn.sigmoid(Wxr @ x + Whr @ h + br)\n",
    "    tildeh = jax.nn.tanh(Wxh @ x + Whh @ (r * h) + bh)\n",
    "    h = (1 - z) * h + z * tildeh\n",
    "    yhat = jax.nn.softmax(Why @ h + by)\n",
    "    return yhat, h\n",
    "\n",
    "def layered_step(params, x, h):\n",
    "    for i in range(len(params)):\n",
    "        x, h[i] = step(params[i], x, h[i])\n",
    "    return x, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(params, x, h):\n",
    "    yhat = np.zeros_like(x)\n",
    "    \n",
    "    for t in range(len(x)):\n",
    "        yhat_t, h = layered_step(params, x[t], h)        \n",
    "        yhat = yhat.at[t, :].set(yhat_t) # equivalent to NumPy's yhat[t, :] = yhat_t\n",
    "\n",
    "    return yhat, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLL(params, x, y, h):\n",
    "    yhat, h = feed_forward(params, x, h)    \n",
    "    loss = -(y * np.log(yhat)).sum()\n",
    "    return loss, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the network parameters so we can test `feed_forward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_size = 100 # number of units in hidden layer\n",
    "\n",
    "def init_params(key):\n",
    "    subkeys = jax.random.split(key, 7)\n",
    "    Wxr = jax.random.normal(subkeys[0], (h_size, vocab_size)) * 0.01 \n",
    "    Whr = jax.random.normal(subkeys[1], (h_size, h_size)) * 0.01\n",
    "    Wxz = jax.random.normal(subkeys[2], (h_size, vocab_size)) * 0.01 \n",
    "    Whz = jax.random.normal(subkeys[3], (h_size, h_size)) * 0.01    \n",
    "    Wxh = jax.random.normal(subkeys[4], (h_size, vocab_size)) * 0.01 \n",
    "    Whh = jax.random.normal(subkeys[5], (h_size, h_size)) * 0.01\n",
    "    Why = jax.random.normal(subkeys[6], (vocab_size, h_size)) * 0.01 \n",
    "    bz = np.zeros(h_size,) \n",
    "    br = np.zeros(h_size,) \n",
    "    bh = np.zeros(h_size,) \n",
    "    by = np.zeros(vocab_size) \n",
    "    params = Wxz, Whz, Wxr, Whr, Wxh, Whh, Why, bz, br, bh, by\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122 ms ± 846 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "sssssssssssssssssssssssss\n",
      "hat, poor contempt, or cl\n",
      "103.17804\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.key(412) # generate new key based on the seed \"42\"\n",
    "init_keys = jax.random.split(key, 3)\n",
    "params = [init_params(k) for k in init_keys]\n",
    "h = [np.zeros(h_size) for _ in range(len(params))]\n",
    "\n",
    "x, y = X[:25], X[1:26]\n",
    "%timeit yhat, _ = feed_forward(params, x, h)\n",
    "yhat, _ = feed_forward(params, x, h)\n",
    "print(onehot_decode(yhat))\n",
    "print(onehot_decode(y))\n",
    "loss, h = NLL(params, x, y, h)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back propagation by automatic differentiation\n",
    "\n",
    "This works in the same way as it did with RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "backprop = jax.value_and_grad(NLL, has_aux=True)\n",
    "\n",
    "(loss, h), grads = backprop(params, x, y, h)\n",
    "for params_i, grads_i in zip(params, grads):\n",
    "    for p, g in zip(params_i, grads_i):\n",
    "        assert p.shape == g.shape\n",
    "        assert not (g == 0).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam optimizer with Optax\n",
    "\n",
    "We can use a JAX implementation of the Adam optimizer from the [Optax](https://optax.readthedocs.io/) library.\n",
    "We first create the optimizer and initialize its state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optax.adam(learning_rate=0.001) # 0.001 is the default from Kingma et al 2014\n",
    "opt_state = optimizer.init(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the optimizer to compute the updates, and apply them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "(loss, h), grads = backprop(params, x, y, h)\n",
    "updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "params = optax.apply_updates(params, updates) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JITing the training step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a function that does all this, and pass it to `jax.jit`, which [just-in-time compiles the function](https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html) so it can be executed efficiently in XLA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit \n",
    "def update_params(params, opt_state, x, y, h):\n",
    "    (loss, h), grads = backprop(params, x, y, h)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, h, opt_state, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.78 ms ± 128 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit update_params(params, opt_state, x, y, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102.981995\n",
      "102.88933\n"
     ]
    }
   ],
   "source": [
    "params, h, opt_state, loss = update_params(params, opt_state, x, y, h)\n",
    "print(loss)\n",
    "params, h, opt_state, loss = update_params(params, opt_state, x, y, h)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling from the network\n",
    "\n",
    "Finally, instead of a `predict` function, we have a `sample` function, which, given the parameters and the number of samples we want, produces a sample of text from the network.\n",
    "\n",
    "It does so by drawing a random seed for $x_0$ and drawing $x_t$ for $t>0$ from the distribution given by $\\widehat y_t$.\n",
    "\n",
    "![](https://www.researchgate.net/profile/Aven-Zhou/publication/337006979/figure/fig3/AS:821430174380045@1572855623911/An-Illustration-of-the-Generating-Sequence-in-an-RNN.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xDnpDH;ETOi\n",
      "t\n",
      "b,cqv;lqejb.QEzMGXCRukBWOoKwNvsiqPzA-nIToZ,FCZlWbVupgOs:BF\n",
      "XclNFzlMpqIKXUVgcPKNGRkax-'\n"
     ]
    }
   ],
   "source": [
    "def sample(params, num_samples, key):\n",
    "    h = [np.zeros(h_size) for _ in range(len(params))]\n",
    "    \n",
    "    x = np.zeros((num_samples, vocab_size), dtype=float)\n",
    "    key, subkey = jax.random.split(key)\n",
    "    seed_char = jax.random.choice(subkey, vocab_size)\n",
    "    x = x.at[0, seed_char].set(1)\n",
    "    \n",
    "    for t in range(1, num_samples):\n",
    "        yhat, h = layered_step(params, x[t-1], h)\n",
    "        # draw from output distribution\n",
    "        key, subkey = jax.random.split(key)\n",
    "        i = jax.random.choice(subkey, vocab_size, p=yhat)\n",
    "        x = x.at[t, i].set(1)\n",
    "    return onehot_decode(x)\n",
    "\n",
    "print(sample(params, 100, jax.random.key(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the network\n",
    "\n",
    "We setup the training - the sequence length to unroll the network, the number of batches, parameter initialization, Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 25\n",
    "max_batches = 1000000\n",
    "h = np.zeros(h_size)\n",
    "pos = 0\n",
    "batch = 0 \n",
    "losses = []\n",
    "key = jax.random.key(8)\n",
    "key, *init_keys = jax.random.split(key)\n",
    "params = [init_params(k) for k in init_keys]\n",
    "h = [np.zeros(h_size) for _ in range(len(params))]\n",
    "\n",
    "optimizer = optax.adam(learning_rate=0.001) # you can try with 0.01\n",
    "opt_state = optimizer.init(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the RNN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0, loss 103.179474, pos 25\n",
      "\n",
      "vdex:k.,onnujDYKgm.qiu;zfjQraswXqj\n",
      "jb-Y-Zkv?nDdos?Ahvw:FOU\n",
      "I,rxqPjmY'wieYpzXpnOuayDqP!QCJv ;PxcfoeqhVIBpbHpDziootlwgiaJt\n",
      "K.TvyEWoAMpBxDrmdlFZznfT- rYBIO\n",
      "Jg.ok,C:Pqxzrtwq?cXZU,xVtLXa,lr-iHARfEI;gCvcp.c\n",
      "--------------------------------------------------------------------------------\n",
      "batch 100000, loss 24.177170, pos 650\n",
      "\n",
      ":\n",
      "Mistress of my soul, my good good fasting well:\n",
      "There ragh lot me what what all them dellinged acdiently times sport of my not be goness and, and\n",
      "I go besterpite a starn by brugh on the soldixed. I'\n",
      "--------------------------------------------------------------------------------\n",
      "batch 200000, loss 30.226427, pos 1275\n",
      "\n",
      "\n",
      "As the muty came Claner.\n",
      "\n",
      "WART OF DARK:\n",
      "Youny he she conceint, on thee, I am but If in this plees;\n",
      "What thou deather as of the soul,\n",
      "I'll whency vouch off Lord;\n",
      "But what is your guiends:\n",
      "I thought 'a\n",
      "--------------------------------------------------------------------------------\n",
      "batch 300000, loss 30.377678, pos 1900\n",
      "\n",
      "XES:\n",
      "Bust a place again with convilly\n",
      "This gold my father's sorn I love\n",
      "And dissloss. Pargure be thouch terrs 's newer have I aid present my an wear envito teach'd:\n",
      "What go to m: being fall not, and i\n",
      "--------------------------------------------------------------------------------\n",
      "batch 400000, loss 28.543024, pos 2525\n",
      "\n",
      "bouthine\n",
      "spought commands our death\n",
      "to make ever that houls to me well ofthiers\n",
      "To every respected. What vistuous incterston.\n",
      "\n",
      "First Saghand talk of her doth begef\n",
      "Thou dost neven sat when he besount'\n",
      "--------------------------------------------------------------------------------\n",
      "batch 500000, loss 12.744192, pos 3150\n",
      "\n",
      "! Come, my lood weed-littect thee, them think in-wo an for what see you are reason,\n",
      "And which banish'd such of it is his earty\n",
      "us revenge,\n",
      "let out and my hath'd to discbedom,\n",
      "In by all thee bett that \n",
      "--------------------------------------------------------------------------------\n",
      "batch 600000, loss 26.552891, pos 3775\n",
      "\n",
      ";\n",
      "Lords I am prefther is Hoviused 'time; wempet\n",
      "To since and morrons me than child:\n",
      "\n",
      "GLOUCESTER:\n",
      "How! I this nate done:\n",
      "Marmat snothicius, and born seems in fore mounter as I am see.\n",
      "\n",
      "GLOUCESTER:\n",
      "To b\n",
      "--------------------------------------------------------------------------------\n",
      "batch 700000, loss 32.970745, pos 4400\n",
      "\n",
      "Duce talis'd is dangerous,\n",
      "Find instee, mo: it nee, with man: but, that greet to dangery.\n",
      "\n",
      "BERTRAM:\n",
      "Liblfil poor and barch, were to very ground and scorrs,\n",
      "Nay, have revid all but it bust\n",
      "Honours with\n",
      "--------------------------------------------------------------------------------\n",
      "batch 800000, loss 30.148172, pos 5025\n",
      "\n",
      "quere the same firm\n",
      "deapo spectainmedy come; and he beer.\n",
      "\n",
      "THO:\n",
      "Give full make me they would lose some entage, in single Lord, and be gons,\n",
      "Nor hirses nqueth imporgan!' oness a deeds and\n",
      "Seaked is, th\n",
      "--------------------------------------------------------------------------------\n",
      "batch 900000, loss 35.270378, pos 5650\n",
      "\n",
      "venge a wind and hath\n",
      "close sea, sweet were go dowed to dispose of holy.\n",
      "\n",
      "VENTIDIUS:\n",
      "No: I was wind; I have made that wish a plutute presently stry\n",
      "Master's terrpresents one on Nhise.\n",
      "\n",
      "CAESAMNIO:\n",
      "Now \n",
      "--------------------------------------------------------------------------------\n",
      "batch 1000000, loss 23.236797, pos 6275\n",
      "\n",
      "Y:\n",
      "Her Murt;\n",
      "Be here, I presenity and daughter to lie every feve\n",
      "my daust with plucken, as let, I was had state, the were prodigh up the sweet,\n",
      "Havings into the ere my wife and all out,\n",
      "And fall'ed be\n",
      "--------------------------------------------------------------------------------\n",
      "CPU times: user 18min 1s, sys: 3min 42s, total: 21min 44s\n",
      "Wall time: 15min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "while batch <= max_batches:\n",
    "    if pos + seq_length + 1 >= data_size:\n",
    "        # reset data position and hidden state\n",
    "        pos, h = 0, [np.zeros(h_size) for _ in range(len(params))]\n",
    "        \n",
    "    x = X[pos : pos + seq_length]\n",
    "    y = X[pos + 1 : pos + seq_length + 1]\n",
    "    pos += seq_length\n",
    "        \n",
    "    params, h, opt_state, loss = update_params(params, opt_state, x, y, h)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    if batch % (max_batches // 10) == 0:\n",
    "        print('batch {:d}, loss {:.6f}, pos {}'.format(batch, loss, pos))\n",
    "        print()\n",
    "        \n",
    "        key, subkey = jax.random.split(key)        \n",
    "        sample_text = sample(params, 200, subkey)\n",
    "        print(sample_text)\n",
    "        print('-'*80)\n",
    "    batch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save / load parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wxz, Whz, Wxr, Whr, Wxh, Whh, Why, bz, br, bh, by = params\n",
    "np.savez(\"../data/gru-jax-params.npz\", Wxz=Wxz, Whz=Whz, Wxr=Wxr, Whr=Whr, Wxh=Wxh, Whh=Whh, Why=Why, bz=bz, br=br, bh=bh, by=by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.load(\"../data/gru-jax-params.npz\")\n",
    "params = d['Wxz'], d['Whz'], d['Wxr'], d['Whr'], d['Wxh'], d['Whh'], d['Why'], d['bz'], d['br'], d['bh'], d['by']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y and\n",
      "The unkoun doth the book, at is\n",
      "She shall tewlifles so much hovest, do a gaid to do!\n",
      "\n",
      "Natter,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sample(params, 100, jax.random.PRNGKey(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- Andrej Karpathy's [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) blogpost\n",
    "- Cho et al. 2014. [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078). arXiv:1406.1078"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colophon\n",
    "This notebook was written by [Yoav Ram](http://python.yoavram.com).\n",
    "\n",
    "This work is licensed under a [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/) International License.\n",
    "\n",
    "![Python logo](https://www.python.org/static/community_logos/python-logo.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

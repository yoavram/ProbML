{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Py4Eng](../logo.png)\n",
    "\n",
    "# Gated  Recurrent Unit\n",
    "## Yoav Ram\n",
    "\n",
    "In this session we will expand over RNN with GRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import jax \n",
    "import jax.numpy as np\n",
    "import optax # pip install optax\n",
    "\n",
    "from collections import Counter\n",
    "from random import uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "In developing this RNN we will follow [Andrej Karpathy](http://cs.stanford.edu/people/karpathy/)'s [blogpost about RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness) ([original code gist](https://gist.github.com/karpathy/d4dee566867f8291f086) with BSD License).\n",
    "\n",
    "The data is just text data, in this case Shakespear's writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 99993\n",
      "Number of unique characters: 62\n",
      "Number of lines: 3298\n",
      "Number of words: 15893\n",
      "\n",
      "Excerpt:\n",
      "********\n",
      "That, poor contempt, or claim'd thou slept so faithful,\n",
      "I may contrive our father; and, in their defeated queen,\n",
      "Her flesh broke me and puttance of expedition house,\n",
      "And in that same that ever I lament this stomach,\n",
      "And he, nor Butly and my fury, knowing everything\n",
      "Grew daily ever, his great strength and thought\n",
      "The bright buds of mine own.\n",
      "\n",
      "BIONDELLO:\n",
      "Marry, that it may not pray their patience.'\n",
      "\n",
      "KING LEAR:\n",
      "The instant common maid, as we may less be\n",
      "a brave gentleman and joiner: he that finds u\n"
     ]
    }
   ],
   "source": [
    "filename = '../data/shakespear.txt'\n",
    "with open(filename, 'rt') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"Number of characters: {}\".format(len(text)))\n",
    "print(\"Number of unique characters: {}\".format(len(set(text))))\n",
    "print(\"Number of lines: {}\".format(text.count('\\n')))\n",
    "print(\"Number of words: {}\".format(text.count(' ')))\n",
    "print()\n",
    "print(\"Excerpt:\")\n",
    "print(\"*\" * len(\"Excerpt:\"))\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating \n",
    "- a list `chars` of the unique characters\n",
    "- `data_size` the number of total characters\n",
    "- `vocab_size` the number of unique characters\n",
    "- `int_to_char` a dictionary from index to char\n",
    "- `char_to_int` a dictionary from char to index\n",
    "and then we convert `data` from a string to a NumPy array of integers representing the chars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = list(set(text))\n",
    "data_size, vocab_size = len(text), len(chars)\n",
    "\n",
    "# char to int and vice versa\n",
    "int_to_char = dict(enumerate(chars)) #  == { i: ch for i,ch in enumerate(chars) }\n",
    "char_to_int = dict(zip(int_to_char.values(), int_to_char.keys())) # { ch: i for i,ch in enumerate(chars) }\n",
    "\n",
    "def onehot_encode(text):\n",
    "    ints = [char_to_int[c] for c in text]\n",
    "    ints = np.array(ints, dtype=int)\n",
    "    return jax.nn.one_hot(ints, vocab_size)\n",
    "\n",
    "def onehot_decode(data):\n",
    "    ints = data.argmax(axis=1).tolist()\n",
    "    chars = (int_to_char[k] for k in ints)\n",
    "    return str.join('', chars)\n",
    "\n",
    "X = onehot_encode(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU model\n",
    "\n",
    "The GRU extends RNN. It avoids the vanishing gradient problem for the vanilla RNN, and is more efficient than LSTM (long-short time memory).\n",
    "To compute the update to the hidden memory layer $h_t$, it first computes a _reset gate_ $r_t$ and an update gate $z_t$ that are used to interpoltate between the candidate memory $\\tilde h_t$ and the next $h_t$.\n",
    "\n",
    "- $x_t$ is the $t$ character, one-hot encoded and a 1D array of length `vocab_size`.\n",
    "- $h_t$ is the state of the hidden memory layer after seeing $t$ characters, encoded as a 1D array of numbers (neurons...)\n",
    "- $r_t$ is the _reset gate_\n",
    "- $z_t$ is the _update gate_\n",
    "- $\\tilde h_t$ is the candidate hidden memory\n",
    "- $\\widehat y_t$ is the prediction of the network after seeing $t$ characters, encoded as a 1D array of probabilities of length `vocab_size`\n",
    "- $\\sigma(x)$ is the sigmoid/logistic function\n",
    "- $\\circ$ is the Hadamard or element-wise product, $x \\circ y = (x_1 y_1, \\ldots x_n y_n)$.\n",
    "\n",
    "The model is then written as:\n",
    "$$\n",
    "z_t = \\sigma{\\left(W_x^z x_t + W_h^z h_{t-1} + b_z\\right)}\n",
    "$$\n",
    "$$\n",
    "r_t = \\sigma{\\left(W_x^r x_t + W_h^r h_{t-1} + b_r\\right)}\n",
    "$$\n",
    "$$\n",
    "\\tilde h_t = \\tanh{\\left(W_x^h x_t + W_h^h (r_t \\circ h_{t-1}) + b_h\\right)}\n",
    "$$\n",
    "$$\n",
    "h_t = (1-z_t) \\circ h_{t-1} + z_t \\circ \\tilde h_t\n",
    "$$\n",
    "$$\n",
    "\\hat y_t = \\mathrm{softmax}\\left(W_h^y h_t + b_y\\right)\n",
    "$$\n",
    "$$\n",
    "x_{t+1} \\sim \\mathrm{Cat}(\\hat{y}_t)\n",
    "$$\n",
    "\n",
    "and we set $h_0 = (0, \\ldots, 0)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This operation will be performed by our `step` function.\n",
    "\n",
    "The `feed_forward` function will loop over a sequence of $x=(x_1, x_2, \\ldots, x_k)$ of some arbitray size - similar to batches in the FFN and CNN frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(params, x, h):\n",
    "    Wxz, Whz, Wxr, Whr, Wxh, Whh, Why, bz, br, bh, by = params\n",
    "    z = jax.nn.sigmoid(Wxz @ x + Whz @ h + bz)\n",
    "    r = jax.nn.sigmoid(Wxr @ x + Whr @ h + br)\n",
    "    tildeh = jax.nn.tanh(Wxh @ x + Whh @ (r * h) + bh)\n",
    "    h = (1 - z) * h + z * tildeh\n",
    "    yhat = jax.nn.softmax(Why @ h + by)\n",
    "    return yhat, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(params, x, h):\n",
    "    yhat = np.zeros_like(x)\n",
    "    \n",
    "    for t in range(len(x)):\n",
    "        yhat_t, h = step(params, x[t], h)        \n",
    "        yhat = yhat.at[t, :].set(yhat_t) # equivalent to NumPy's yhat[t, :] = yhat_t\n",
    "\n",
    "    return yhat, h\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLL(params, x, y, h):\n",
    "    yhat, h = feed_forward(params, x, h)    \n",
    "    loss = -(y * np.log(yhat)).sum()\n",
    "    return loss, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the network parameters so we can test `feed_forward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_size = 100 # number of units in hidden layer\n",
    "\n",
    "def init_params(key):\n",
    "    subkeys = jax.random.split(key, 7)\n",
    "    Wxr = jax.random.normal(subkeys[0], (h_size, vocab_size)) * 0.01 \n",
    "    Whr = jax.random.normal(subkeys[1], (h_size, h_size)) * 0.01\n",
    "    Wxz = jax.random.normal(subkeys[2], (h_size, vocab_size)) * 0.01 \n",
    "    Whz = jax.random.normal(subkeys[3], (h_size, h_size)) * 0.01    \n",
    "    Wxh = jax.random.normal(subkeys[4], (h_size, vocab_size)) * 0.01 \n",
    "    Whh = jax.random.normal(subkeys[5], (h_size, h_size)) * 0.01\n",
    "    Why = jax.random.normal(subkeys[6], (vocab_size, h_size)) * 0.01 \n",
    "    bz = np.zeros(h_size,) \n",
    "    br = np.zeros(h_size,) \n",
    "    bh = np.zeros(h_size,) \n",
    "    by = np.zeros(vocab_size) \n",
    "    params = Wxz, Whz, Wxr, Whr, Wxh, Whh, Why, bz, br, bh, by\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.1 ms ± 1.33 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "DQVQVK FzzKmznQtuuQVK,iKm\n",
      "hat, poor contempt, or cl\n",
      "103.17714\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.key(42) # generate new key based on the seed \"42\"\n",
    "params = init_params(key)\n",
    "\n",
    "x, y = X[:25], X[1:26]\n",
    "h = np.zeros(h_size)\n",
    "%timeit yhat, _ = feed_forward(params, x, h)\n",
    "yhat, _ = feed_forward(params, x, h)\n",
    "print(onehot_decode(yhat))\n",
    "print(onehot_decode(y))\n",
    "loss, h = NLL(params, x, y, h)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back propagation by automatic differentiation\n",
    "\n",
    "This works in the same way as it did with RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "backprop = jax.value_and_grad(NLL, has_aux=True)\n",
    "\n",
    "(loss, h), grads = backprop(params, x, y, h)\n",
    "for p, g in zip(params, grads):\n",
    "    assert p.shape == g.shape\n",
    "    assert not (g == 0).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam optimizer with Optax\n",
    "\n",
    "We can use a JAX implementation of the Adam optimizer from the [Optax](https://optax.readthedocs.io/) library.\n",
    "We first create the optimizer and initialize its state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optax.adam(learning_rate=0.001) # 0.001 is the default from Kingma et al 2014\n",
    "opt_state = optimizer.init(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the optimizer to compute the updates, and apply them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "(loss, h), grads = backprop(params, x, y, h)\n",
    "updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "params = optax.apply_updates(params, updates) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JITing the training step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a function that does all this, and pass it to `jax.jit`, which [just-in-time compiles the function](https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html) so it can be executed efficiently in XLA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit \n",
    "def update_params(params, opt_state, x, y, h):\n",
    "    (loss, h), grads = backprop(params, x, y, h)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, h, opt_state, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163 μs ± 21.4 μs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit update_params(params, opt_state, x, y, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103.10628\n",
      "103.03173\n"
     ]
    }
   ],
   "source": [
    "params, h, opt_state, loss = update_params(params, opt_state, x, y, h)\n",
    "print(loss)\n",
    "params, h, opt_state, loss = update_params(params, opt_state, x, y, h)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling from the network\n",
    "\n",
    "Finally, instead of a `predict` function, we have a `sample` function, which, given the parameters and the number of samples we want, produces a sample of text from the network.\n",
    "\n",
    "It does so by drawing a random seed for $x_0$ and drawing $x_t$ for $t>0$ from the distribution given by $\\widehat y_t$.\n",
    "\n",
    "![](https://www.researchgate.net/profile/Aven-Zhou/publication/337006979/figure/fig3/AS:821430174380045@1572855623911/An-Illustration-of-the-Generating-Sequence-in-an-RNN.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iYsaYO'MdcnDlDbCEtA'JtGBbzxMQZK?qh;Srvc-ogNAHntkQWfsLd-PCwqPJvbu;aXcHRrwD?EJNwQJZatLo?:uXEkoNKhSmif\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def sample(params, num_samples, key):\n",
    "    h = np.zeros(h_size)\n",
    "    \n",
    "    x = np.zeros((num_samples, vocab_size), dtype=float)\n",
    "    key, subkey = jax.random.split(key)\n",
    "    seed_char = jax.random.choice(subkey, vocab_size)\n",
    "    x = x.at[0, seed_char].set(1)\n",
    "    \n",
    "    for t in range(1, num_samples):\n",
    "        yhat, h = step(params, x[t-1], h)\n",
    "        # draw from output distribution\n",
    "        key, subkey = jax.random.split(key)\n",
    "        i = jax.random.choice(subkey, vocab_size, p=yhat)\n",
    "        x = x.at[t, i].set(1)\n",
    "    return onehot_decode(x)\n",
    "\n",
    "print(sample(params, 100, jax.random.PRNGKey(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the network\n",
    "\n",
    "We setup the training - the sequence length to unroll the network, the number of batches, parameter initialization, Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 25\n",
    "max_batches = 1000000\n",
    "h = np.zeros(h_size)\n",
    "pos = 0\n",
    "batch = 0 \n",
    "losses = []\n",
    "key = jax.random.PRNGKey(8)\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "params = init_params(subkey)\n",
    "\n",
    "optimizer = optax.adam(learning_rate=0.001) # you can try with 0.01\n",
    "opt_state = optimizer.init(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the RNN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0, loss 103.178619, pos 25\n",
      "\n",
      "AVGiRSzC-ss;BYFoX.ztn;'QTBxpmHg?tBDBbfFfPSAUsYV-HUW AgRwc:DLCpitkB.F\n",
      "gnGFaQ?asc;m,YtkIxqeAj'kiET-Gt uLrabOaYQn--lJgXnmelDozdA,Mv-WZariYp.VJwPQsTdfjpFrLcDeXz-SCqRktiQplgtUE?P:Ciul!?mCJpfnOWhTML'XqAEazE\n",
      "--------------------------------------------------------------------------------\n",
      "batch 100000, loss 26.178045, pos 650\n",
      "\n",
      "RSWOLK:\n",
      "I stom the why at a rabour thus ever.\n",
      "How'd abonts: spall I doth nobly, Huched of head thus ender, Let us that ham a morome other puatchine.\n",
      "\n",
      "First Servrons, pass of thy; ands unders antry buc\n",
      "--------------------------------------------------------------------------------\n",
      "batch 200000, loss 33.671314, pos 1275\n",
      "\n",
      "DFint you,\n",
      "And dreath.\n",
      "\n",
      "BERTRAN:\n",
      "I would that heart great practises their rude;\n",
      "With reason?\n",
      "\n",
      "BORTO:\n",
      "Thanks?\n",
      "\n",
      "COUBATAN:\n",
      "For she hast now, fearders\n",
      "She had be so: I am aften thing flodey whor a beart\n",
      "A\n",
      "--------------------------------------------------------------------------------\n",
      "batch 300000, loss 27.475714, pos 1900\n",
      "\n",
      "?\n",
      "\n",
      "ESCA:\n",
      "She attent hath tarter,\n",
      "But and me threate?\n",
      "\n",
      "EARL OF GEAARIUS:\n",
      "Sir, I will glore yourself son, and lict been and with dead, and seen\n",
      "And lost him, in evare.\n",
      "\n",
      "HARMINGAN:\n",
      "Then are her sicks I a\n",
      "--------------------------------------------------------------------------------\n",
      "batch 400000, loss 28.998547, pos 2525\n",
      "\n",
      "basbed on the more thus in in thy before,\n",
      "In love that that parded's exprssed in morture lefarry add appives in the rither hearcte be we break\n",
      "not a stare\n",
      "To come loed,\n",
      "When he that honour of it bline\n",
      "--------------------------------------------------------------------------------\n",
      "batch 500000, loss 19.543644, pos 3150\n",
      "\n",
      "I warl and pheat:\n",
      "Now, for my shake And leavy fought; but to know me,\n",
      "Told: such a foold kinders your fair Tergent's ance merbable sween say haste kince is is\n",
      "That yet cantames up, she may them their \n",
      "--------------------------------------------------------------------------------\n",
      "batch 600000, loss 27.108488, pos 3775\n",
      "\n",
      "'d my soul,\n",
      "Let it a herceful soul\n",
      "Yer eart.\n",
      "Yor in earthful, and break this fepheign; and inducested beliends,\n",
      "The how love it lets I may swear sovereen a so a serme.\n",
      "\n",
      "ROSALIND:\n",
      "We makes fool my daug\n",
      "--------------------------------------------------------------------------------\n",
      "batch 700000, loss 32.881187, pos 4400\n",
      "\n",
      "Y, Goodly contemnine.\n",
      "\n",
      "GLOUCESTER:\n",
      "They are go to you:\n",
      "\n",
      "ANTONY, ALaster, voats me in thy bodoks revendion.\n",
      "\n",
      "KING LEAR:\n",
      "Yet, gruave I should do the fawOled every I\n",
      "belitines as impony-SleUd:\n",
      "Hust and p\n",
      "--------------------------------------------------------------------------------\n",
      "batch 800000, loss 31.077095, pos 5025\n",
      "\n",
      "ther: and his madre: more to Cirron's; and faish.\n",
      "\n",
      "DECBRUGHELS:\n",
      "He doubled ingendow'd, a'liot trumpering them, I fear at your dueps\n",
      "To my\n",
      "To thee in thee, Sleedy a too, their pray them and brother; an\n",
      "--------------------------------------------------------------------------------\n",
      "batch 900000, loss 33.232025, pos 5650\n",
      "\n",
      "Ad of meed,\n",
      "With singer.\n",
      "\n",
      "FORD:\n",
      "I am Edging:\n",
      "More: not bed!\n",
      "As I shall refory, a trervoudy I begeted woulder bend your geat\n",
      "But iloud be spoin.\n",
      "\n",
      "ALBANY:\n",
      "Nhe behind thee; I am fleed in the will lose th\n",
      "--------------------------------------------------------------------------------\n",
      "batch 1000000, loss 23.376553, pos 6275\n",
      "\n",
      "Fat: these\n",
      "Nor ears in the san lost so would not yeen and bloody: why, must my requerate,\n",
      "Of Murter,--\n",
      "\n",
      "FALSTAFF:\n",
      "Then made her prince his good things and, Good O, I'll not bask but it was storack'ss \n",
      "--------------------------------------------------------------------------------\n",
      "CPU times: user 53min 23s, sys: 40min 4s, total: 1h 33min 27s\n",
      "Wall time: 45min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "while batch <= max_batches:\n",
    "    if pos + seq_length + 1 >= data_size:\n",
    "        # reset data position and hidden state\n",
    "        pos, h = 0, np.zeros(h_size) \n",
    "        \n",
    "    x = X[pos : pos + seq_length]\n",
    "    y = X[pos + 1 : pos + seq_length + 1]\n",
    "    pos += seq_length\n",
    "        \n",
    "    params, h, opt_state, loss = update_params(params, opt_state, x, y, h)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    if batch % (max_batches // 10) == 0:\n",
    "        print('batch {:d}, loss {:.6f}, pos {}'.format(batch, loss, pos))\n",
    "        print()\n",
    "        \n",
    "        key, subkey = jax.random.split(key)        \n",
    "        sample_text = sample(params, 200, subkey)\n",
    "        print(sample_text)\n",
    "        print('-'*80)\n",
    "    batch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save / load parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wxz, Whz, Wxr, Whr, Wxh, Whh, Why, bz, br, bh, by = params\n",
    "np.savez(\"../data/gru-jax-params.npz\", Wxz=Wxz, Whz=Whz, Wxr=Wxr, Whr=Whr, Wxh=Wxh, Whh=Whh, Why=Why, bz=bz, br=br, bh=bh, by=by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.load(\"../data/gru-jax-params.npz\")\n",
    "params = d['Wxz'], d['Whz'], d['Wxr'], d['Whr'], d['Wxh'], d['Whh'], d['Why'], d['bz'], d['br'], d['bh'], d['by']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y and\n",
      "The unkoun doth the book, at is\n",
      "She shall tewlifles so much hovest, do a gaid to do!\n",
      "\n",
      "Natter,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sample(params, 100, jax.random.PRNGKey(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- Andrej Karpathy's [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) blogpost\n",
    "- Cho et al. 2014. [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078). arXiv:1406.1078"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colophon\n",
    "This notebook was written by [Yoav Ram](http://python.yoavram.com).\n",
    "\n",
    "This work is licensed under a [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/) International License.\n",
    "\n",
    "![Python logo](https://www.python.org/static/community_logos/python-logo.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DataSciPy]",
   "language": "python",
   "name": "conda-env-DataSciPy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
